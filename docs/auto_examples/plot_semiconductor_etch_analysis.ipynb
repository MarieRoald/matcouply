{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# PARAFAC2 for semiconductor etch analysis\n\nComponent models have been used to detect errors in semiconductor etch processes :cite:p:`wise1999comparison`, where the\ndatasets have three natural modes: sample, measurement and time. However, the time required for measuring\ndifferent samples may vary, which leads to a stack of matrices, one for each sample. This makes PARAFAC2 a\nnatural choice :cite:p:`wise2001application`, as it naturally handles time profiles of different lengths.\n\nIn this example, we repeat some of the analysis from :cite:p:`wise2001application` and show how total variation (TV) regularization\ncan reduce noise in the components. TV regularization is well suited for reducing noise without overly smoothing\nsharp transitions :cite:p:`rudin1992nonlinear`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorly.decomposition import parafac2\n\nfrom matcouply.data import get_semiconductor_etch_machine_data\nfrom matcouply.decomposition import parafac2_aoadmm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading and preprocessing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_data, train_metadata, test_data, test_metadata = get_semiconductor_etch_machine_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset contains three experiments (experiments 29, 31 and 33). In :cite:p:`wise2001application`, the authors\nhighlight the components obtained for experiment 29, so let's look at the same experiment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_29 = [key for key in train_data if key.isnumeric() and key.startswith(\"29\")]\nmatrices = [train_data[key].values for key in dataset_29]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before analysing, we apply the same preprocessing steps as in :cite:p:`wise2001application` \u2014 centering and scaling each\nmatrix based on the global mean and standard deviation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stacked = np.concatenate(matrices, axis=0)\nmean = stacked.mean(0, keepdims=True)\nstd = stacked.std(0, keepdims=True)\nstandardised = [(m - mean) / std for m in matrices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit a PARAFAC2 model\n\nLet's begin by fitting an unregularized PARAFAC2 model using the alternating least squares algorithm\n:cite:p:`kiers1999parafac2` with the implementation in `TensorLy <http://tensorly.org/>`_ :cite:p:`kossaifi2019tensorly`.\nThis algorithm is comparable with the one used in :cite:p:`wise2001application`.\n\nWe also impose non-negativity on the $\\mathbf{A}$-matrix to handle the special sign indeterminacy of\nPARAFAC2 :cite:p:`harshman1972parafac2`. The $\\mathbf{A}$-matrix elements in :cite:p:`wise2001application` are\nalso non-negative, so this shouldn't change the components.\n\nSimilarly as :cite:`wise2001application`, we extract two components.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pf2, rec_err = parafac2(\n    standardised, 2, n_iter_max=10_000, return_errors=True, nn_modes=[0], random_state=0, tol=1e-9, verbose=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We examine the results by plotting the relative SSE and its relative change as a function of iteration number\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "it_num = np.arange(len(rec_err)) + 1\nrel_sse = np.array(rec_err) ** 2\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 3), tight_layout=True)\naxes[0].plot(it_num, rel_sse)\naxes[0].set_ylim(0.67, 0.68)\naxes[0].set_xlabel(\"Iteration number\")\naxes[0].set_ylabel(\"Relative SSE\")\n\naxes[1].semilogy(it_num[1:], (rel_sse[:-1] - rel_sse[1:]) / rel_sse[:-1])\naxes[1].set_xlabel(\"Iteration number\")\naxes[1].set_ylabel(\"Relative change in SSE\")\naxes[1].set_ylim(1e-9, 1e-6)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we look at the components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights, (A, B, C), P_is = pf2\nB_is = [P_i @ B for P_i in P_is]\n\n# We normalise the components to make them easier to compare\nA_norm = np.linalg.norm(A, axis=0, keepdims=True)\nC_norm = np.linalg.norm(C, axis=0, keepdims=True)\nA = A / A_norm\nB_is = [B_i * A_norm * C_norm for B_i in B_is]\nC = C / C_norm\n\n# We find the permutation so the first component explains most of the variation in the data\nB_norm = np.linalg.norm(B, axis=0, keepdims=True)\npermutation = np.argsort(weights * A_norm * B_norm * C_norm).squeeze()\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\nfor i, B_i in enumerate(B_is):\n    axes[0].plot(B_i[:, permutation[0]], color=\"k\", alpha=0.3)\n    axes[1].plot(B_i[:, permutation[1]], color=\"k\", alpha=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the components are similar to those in :cite:p:`wise2001application`. We can see an overall shape, but\nthey are fairly noisy.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In this simple example, we only use one random initialisation. For a more thorough analysis, you should fit\n    several models with different random initialisations and select the model with the lowest SSE\n    :cite:p:`yu2021parafac2`.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next we use PARAFAC2 ADMM to apply a TV penalty\n\nSince the TV-penalty scales with the norm of the factors, we also need to penalise the norm of $\\mathbf{A}$\nand $\\mathbf{C}$ :cite:p:`roald2021admm`. In this case, we use unit ball constraints, constraining the columns of\n$\\mathbf{A}$ and $\\mathbf{C}$ to have unit norm.\n\nSimilar as before, we add non-negativity on $\\mathbf{A}$ to resolve the sign indeterminacy.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The proximal operator for the total variation penalty is computed using the C-implementation for the improved\n    version of the direct TV algorithm presented in :cite:p:`condat2013direct`. The C-implementation is CeCILL\n    lisenced and is available `here <https://lcondat.github.io/software.html>`__, and the Python-wrapper,\n    `condat-tv`, is GPL-3 lisenced and is available `here <https://github.com/MarieRoald/condat_tv>`__.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cmf, diagnostics = parafac2_aoadmm(\n    standardised,\n    2,\n    n_iter_max=10_000,\n    non_negative={0: True},\n    l2_norm_bound=[1, None, 1],\n    tv_penalty={1: 0.1},\n    verbose=100,\n    return_errors=True,\n    init_params={\"nn_modes\": [0]},\n    constant_feasibility_penalty=True,\n    tol=1e-9,\n    random_state=0,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We examine the diagnostic plots\n\nFor ALS, the relative SSE and its change was the only interesting metrics. However, with regularized PARAFAC2 and AO-ADMM\nwe should also to look at the feasibility gaps and the regularization penalty.\n\nAll feasibility gaps and the change in relative SSE should be low.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rel_sse = np.array(diagnostics.rec_errors) ** 2\nloss = np.array(diagnostics.regularized_loss)\nfeasibility_penalty_A = np.array([gapA for gapA, gapB, gapC in diagnostics.feasibility_gaps])\nfeasibility_penalty_B = np.array([gapB for gapA, gapB, gapC in diagnostics.feasibility_gaps])\nfeasibility_penalty_C = np.array([gapC for gapA, gapB, gapC in diagnostics.feasibility_gaps])\n\nit_num = np.arange(len(rel_sse))\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 6), tight_layout=True)\naxes[0, 0].plot(it_num, rel_sse)\naxes[0, 0].set_ylim(0.69, 0.71)\naxes[0, 0].set_xlabel(\"Iteration number\")\naxes[0, 0].set_ylabel(\"Relative SSE\")\n\naxes[0, 1].plot(it_num, loss)\naxes[0, 1].set_xlabel(\"Iteration number\")\naxes[0, 1].set_ylabel(\"Regularized loss\")\n\naxes[0, 2].semilogy(it_num[1:], np.abs(loss[:-1] - loss[1:]) / loss[:-1])\naxes[0, 2].set_xlabel(\"Iteration number\")\naxes[0, 2].set_ylabel(\"Relative change in regularized loss\")\n\naxes[1, 0].semilogy(it_num, feasibility_penalty_A)\naxes[1, 0].set_xlabel(\"Iteration number\")\naxes[1, 0].set_ylabel(\"Feasibility gap A\")\n\naxes[1, 1].semilogy(it_num, feasibility_penalty_B)\naxes[1, 1].set_xlabel(\"Iteration number\")\naxes[1, 1].set_ylabel(\"Feasibility gap B_is\")\naxes[1, 1].legend([\"PARAFAC2\", \"TV\"])\n\naxes[1, 2].semilogy(it_num, feasibility_penalty_C)\naxes[1, 2].set_xlabel(\"Iteration number\")\naxes[1, 2].set_ylabel(\"Feasibility gap C\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we look at the regularized components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights, (A, B_is, C) = cmf\n# We find the permutation so the first component explains most of the variation in the data\nB_norm = np.linalg.norm(B_is[0], axis=0, keepdims=True)  # All B_is have same norm due to PARAFAC2 constraint\npermutation = np.argsort(B_norm).squeeze()\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\nfor i, B_i in enumerate(B_is):\n    axes[0].plot(B_i[:, permutation[0]], color=\"k\", alpha=0.3)\n    axes[1].plot(B_i[:, permutation[1]], color=\"k\", alpha=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the TV regularization removed much of the noise. We now have piecewise constant components\nwith transitions that are easy to identify.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing with unregularized PARAFAC2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Relative SSE with unregularized PARAFAC2: \", rec_err[-1] ** 2)\nprint(\"Relative SSE with TV regularized PARAFAC2:\", diagnostics.rec_errors[-1] ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there is only a small change in the relative SSE, but the components are much smoother and\nthe transitions are clearer.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## License\n\nSince this example uses the `condat_tv`-library, it is lisenced under a GPL-3 license\n\n.. code:: text\n\n                      Version 3, 29 June 2007\n\n    Example demonstrating TV regularized PARAFAC2\n    Copyright (C) 2021 Marie Roald\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}