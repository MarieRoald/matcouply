{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Example with custom penalty class for unimodality for all but one component\n\nIn this example, we first demonstrate how to specify exactly how the penalties are imposed in the AO-ADMM fitting\nprocedure. Then, we create a custom penalty that imposes non-negativity on all component vectors and unimodality on all\nbut one of the component vectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nimport tensorly as tl\nfrom component_vis.factor_tools import factor_match_score\n\nimport matcouply.decomposition as decomposition\nfrom matcouply.coupled_matrices import CoupledMatrixFactorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "I, J, K = 10, 40, 15\nrank = 3\nnoise_level = 0.2\nrng = np.random.default_rng(0)\n\n\ndef normalize(x):\n    return x / tl.sqrt(tl.sum(x ** 2, axis=0, keepdims=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate simulated data that follows the PARAFAC2 constraint\nWe start by generating some components, for the $\\mathbf{A}$ and $\\mathbf{C}$ components, we use uniformly\ndistributed component vector elements. For the $\\mathbf{B}^{(i)}$-components, we create two unimodal vectors and one\ncomponent vector with uniformly distributed elements, and shift these vectors for each $i$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Random uniform components\nA = rng.uniform(size=(I, rank)) + 0.1  # Add 0.1 to ensure that there is signal for all components for all slices\nA = tl.tensor(A)\n\nB_0 = tl.zeros((J, rank))\n# Simulating unimodal components\nt = np.linspace(-10, 10, J)\nfor r in range(rank - 1):\n    sigma = rng.uniform(0.5, 1.5)\n    mu = rng.uniform(-10, 0)\n    B_0[:, r] = stats.norm.pdf(t, loc=mu, scale=sigma)\n# The final component is random uniform, not unimodal\nB_0[:, rank - 1] = rng.uniform(size=(J,))\n\n# Shift the components for each slice\nB_is = [np.roll(B_0, i, axis=0) for i in range(I)]\nB_is = [tl.tensor(B_i) for B_i in B_is]\n\n\n# Random uniform components\nC = rng.uniform(size=(K, rank))\nC = tl.tensor(C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the simulated components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, tight_layout=True)\n\naxes[0, 0].plot(normalize(A))\naxes[0, 0].set_title(\"$\\\\mathbf{A}$\")\n\naxes[0, 1].plot(normalize(C))\naxes[0, 1].set_title(\"$\\\\mathbf{C}$\")\n\naxes[0, 2].axis(\"off\")\n\naxes[1, 0].plot(normalize(B_is[0]))\naxes[1, 0].set_title(\"$\\\\mathbf{B}_0$\")\n\naxes[1, 1].plot(normalize(B_is[I // 2]))\naxes[1, 1].set_title(f\"$\\\\mathbf{{B}}_{{{I//2}}}$\")\n\naxes[1, 2].plot(normalize(B_is[-1]))\naxes[1, 2].set_title(f\"$\\\\mathbf{{B}}_{{{I-1}}}$\")\nfig.legend([\"Component 0\", \"Component 1\", \"Component 2\"], bbox_to_anchor=(0.95, 0.75), loc=\"center right\")\nfig.suptitle(\"Simulated components\")\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the $\\mathbf{B}^{(i)}$-s, we see that component 0 and 1 are unimodal, while component 2 is not.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the coupled matrix factorization, simulated data matrices and add noise\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cmf = CoupledMatrixFactorization((None, (A, B_is, C)))\nmatrices = cmf.to_matrices()\nnoise = [tl.tensor(rng.uniform(size=M.shape)) for M in matrices]\nnoisy_matrices = [M + N * noise_level * tl.norm(M) / tl.norm(N) for M, N in zip(matrices, noise)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the ``regs`` parameter to input regularization classes\nMatcouply automatically parses the constraints from the ``parafac2_aoadmm`` and ``cmf_aoadmm`` funciton\narguments. However, sometimes, you may want full control over how a penalty is implemented. In that case,\nthe ``regs``-argument is useful. This argument makes it possible to specify exactly which penalty instances\nto use.\n\nSince the components are non-negative, it makes sense to fit a non-negative PARAFAC2 model, however,\nwe also know that two of the $\\mathbf{B}^{(i)}$-component vectors are unimodal, so we first try with\na fully unimodal decomposition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcouply.penalties import NonNegativity, Unimodality\n\nlowest_error = float(\"inf\")\nfor init in range(4):\n    print(\"Init:\", init)\n    out = decomposition.parafac2_aoadmm(\n        noisy_matrices,\n        rank,\n        n_iter_max=1000,\n        regs=[[NonNegativity()], [Unimodality(non_negativity=True)], [NonNegativity()]],\n        return_errors=True,\n        random_state=init,\n        verbose=True,\n    )\n    if out[1].regularized_loss[-1] < lowest_error and out[1].satisfied_stopping_condition:\n        out_cmf, diagnostics = out\n        lowest_error = diagnostics.rec_errors[-1]\n\nprint(\"=\" * 50)\nprint(f\"Final reconstruction error: {lowest_error:.3f}\")\nprint(f\"Feasibility gap for A: {diagnostics.feasibility_gaps[-1][0]}\")\nprint(f\"Feasibility gap for B_is: {diagnostics.feasibility_gaps[-1][1]}\")\nprint(f\"Feasibility gap for C: {diagnostics.feasibility_gaps[-1][2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute factor match score to measure the accuracy of the recovered components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_stacked_CP_tensor(cmf):\n    weights, factors = cmf\n    A, B_is, C = factors\n\n    stacked_cp_tensor = (weights, (A, np.concatenate(B_is, axis=0), C))\n    return stacked_cp_tensor\n\n\nfms, permutation = factor_match_score(\n    get_stacked_CP_tensor(cmf), get_stacked_CP_tensor(out_cmf), consider_weights=False, return_permutation=True\n)\nprint(f\"Factor match score: {fms}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the recovered components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "out_weights, (out_A, out_B_is, out_C) = out_cmf\nout_A = out_A[:, permutation]\nout_B_is = [out_B_i[:, permutation] for out_B_i in out_B_is]\nout_C = out_C[:, permutation]\n\nfig, axes = plt.subplots(2, 3, tight_layout=True)\n\naxes[0, 0].plot(normalize(out_A))\naxes[0, 0].set_title(\"$\\\\mathbf{A}$\")\n\naxes[0, 1].plot(normalize(out_C))\naxes[0, 1].set_title(\"$\\\\mathbf{C}$\")\n\naxes[0, 2].axis(\"off\")\n\naxes[1, 0].plot(normalize(out_B_is[0]))\naxes[1, 0].set_title(\"$\\\\mathbf{B}_0$\")\n\naxes[1, 1].plot(normalize(out_B_is[I // 2]))\naxes[1, 1].set_title(f\"$\\\\mathbf{{B}}_{{{I//2}}}$\")\n\naxes[1, 2].plot(normalize(out_B_is[-1]))\naxes[1, 2].set_title(f\"$\\\\mathbf{{B}}_{{{I-1}}}$\")\nfig.legend([\"Component 0\", \"Component 1\", \"Component 2\"], bbox_to_anchor=(0.95, 0.75), loc=\"center right\")\n\nfig.suptitle(r\"Unimodality on the $\\mathbf{B}^{(i)}$-components\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the $\\mathbf{C}$-component vectors all follow the same pattern and that the the\n$\\mathbf{A}$-component vectors all follow a similar pattern. This is not the case with the real,\nuncorrelated random, components. The $\\mathbf{B}^{(i)}$-component vectors also follow a strange pattern\nwith peaks jumping forwards and backwards, which we know are not the case with the real components either.\n\nHowever, this strange behaviour is not too surprising, considering that there are only two uniomdal component\nvectors in the data. So this model that assumes all unimodal components might be too restrictive.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a custom penalty class for unimodality in all but one class\nNow, we want to make a custom penalty that imposes unimodality on the first $R-1$ component\nvectors. Since unimodality are imposed column-wise, we know that this constraint is a matrix penalty\n(as opposed to a row-vector penalty or a multi-matrix penalty), so we import the ``MatrixPenalty``-superclass\nfrom ``matcouply.penalties``. We also know that unimodality is a hard constraint, so we import\nthe ``HardConstraintMixin``-class, which provides a ``penalty``-method that always returns 0 and has an informative\ndocstring.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcouply._doc_utils import (\n    copy_ancestor_docstring,  # Helper decorator that makes it possible for ADMMPenalties to inherit a docstring\n)\nfrom matcouply._unimodal_regression import unimodal_regression  # The unimodal regression implementation\nfrom matcouply.penalties import HardConstraintMixin, MatrixPenalty\n\n\nclass CustomUnimodality(HardConstraintMixin, MatrixPenalty):\n    def __init__(self, non_negativity=False, aux_init=\"random_uniform\", dual_init=\"random_uniform\"):\n        super().__init__(aux_init, dual_init)\n        self.non_negativity = non_negativity\n\n    @copy_ancestor_docstring\n    def factor_matrix_update(self, factor_matrix, feasibility_penalty, aux):\n        new_factor_matrix = tl.copy(factor_matrix)\n        new_factor_matrix[:, :-1] = unimodal_regression(factor_matrix[:, :-1], non_negativity=self.non_negativity)\n        if self.non_negativity:\n            new_factor_matrix = tl.clip(new_factor_matrix, 0)\n        return new_factor_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit a non-negative PARAFAC2 model using the custom penalty class on the B mode\nNow, we can fit a new model with the custom unimodality class\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lowest_error = float(\"inf\")\nfor init in range(4):\n    print(\"Init:\", init)\n    out = decomposition.parafac2_aoadmm(\n        noisy_matrices,\n        rank,\n        n_iter_max=1000,\n        regs=[[NonNegativity()], [CustomUnimodality(non_negativity=True)], [NonNegativity()]],\n        return_errors=True,\n        random_state=init,\n        verbose=True,\n    )\n    if out[1].regularized_loss[-1] < lowest_error and out[1].satisfied_stopping_condition:\n        out_cmf, diagnostics = out\n        lowest_error = diagnostics.rec_errors[-1]\n\nprint(\"=\" * 50)\nprint(f\"Final reconstruction error: {lowest_error:.3f}\")\nprint(f\"Feasibility gap for A: {diagnostics.feasibility_gaps[-1][0]}\")\nprint(f\"Feasibility gap for B_is: {diagnostics.feasibility_gaps[-1][1]}\")\nprint(f\"Feasibility gap for C: {diagnostics.feasibility_gaps[-1][2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute factor match score to measure the accuracy of the recovered components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fms, permutation = factor_match_score(\n    get_stacked_CP_tensor(cmf), get_stacked_CP_tensor(out_cmf), consider_weights=False, return_permutation=True\n)\nprint(f\"Factor match score: {fms}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the factor match score is much better now compared to before!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the recovered components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "out_weights, (out_A, out_B_is, out_C) = out_cmf\nout_A = out_A[:, permutation]\nout_B_is = [out_B_i[:, permutation] for out_B_i in out_B_is]\nout_C = out_C[:, permutation]\n\nfig, axes = plt.subplots(2, 3, tight_layout=True)\n\naxes[0, 0].plot(normalize(out_A))\naxes[0, 0].set_title(\"$\\\\mathbf{A}$\")\n\naxes[0, 1].plot(normalize(out_C))\naxes[0, 1].set_title(\"$\\\\mathbf{C}$\")\n\naxes[0, 2].axis(\"off\")\n\naxes[1, 0].plot(normalize(out_B_is[0]))\naxes[1, 0].set_title(\"$\\\\mathbf{B}_0$\")\n\naxes[1, 1].plot(normalize(out_B_is[I // 2]))\naxes[1, 1].set_title(f\"$\\\\mathbf{{B}}_{{{I//2}}}$\")\n\naxes[1, 2].plot(normalize(out_B_is[-1]))\naxes[1, 2].set_title(f\"$\\\\mathbf{{B}}_{{{I-1}}}$\")\nfig.legend([\"Component 0\", \"Component 1\", \"Component 2\"], bbox_to_anchor=(0.95, 0.75), loc=\"center right\")\nfig.suptitle(r\"Custom uniomdality on the $\\mathbf{B}^{(i)}$-components\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the model finds much more sensible component vectors. The $\\mathbf{A}$- and\n$\\mathbf{C}$-component vectors no longer seem correlated, and the peaks of the $\\mathbf{B}^{(i)}$-component\nvectors no longer jump around.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}